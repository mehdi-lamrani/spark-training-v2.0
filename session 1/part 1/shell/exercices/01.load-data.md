# LOADING DATA INTO HDFS

SSH into your master instance

### A quick course about HDFS will be given in the Next Session
For now, all you need to know is that HDFS is a distributed File System,   
meaning the data you load into it is chopped in bits across all servers of your cluster.   
You need to download your data locally on your Master server, and then load it into HDFS   
to make it available to Spark from all Nodes from any executor

**DO NOT SKIP/MISS ANY STEP AND FOLLOW EVERY INSTRUCTION CAREFULLY**

- To avoid many inconveniences due to permissions, sudo as root  
  ALL Exercices starting from now should be done from the root user
  Not respecting this preliminary step in the next exercices can result in errors
**of course, this is totally forbidden in production environment**

```
sudo su
cd
```


- create some local working directories

```
mkdir yourfirstname (i.e bob)
cd yourfirstname
mkdir data 
cd data
```

- download content
```
wget https://raw.githubusercontent.com/vega/vega-datasets/master/data/zipcodes.csv
cat zipcodes.csv
```

- create folders in HDFS & upload content into it

Mind your copy&pasting :  
Replace `putyourfirstnamehere` by your name

```
hdfs dfs -mkdir /user/root/yourfirstname/
hdfs dfs -mkdir /user/root/yourfirstname/data/
hdfs dfs -put zipcodes.csv /user/root/yourfirstname/data/
```

- check it is ok :
```
hdfs dfs -ls /user/root/yourfirstname/data/
```
You should see the following output  

````
Found 1 items
-rw-r--r--   1 root hadoop    2018388 2020-11-25 22:11 /user/root/yourfirstname/data/zipcodes.csv
````


Now let's play with this toy dataset in a Spark Shell

- [Hello Pyspark Shell](https://github.com/mehdi-lamrani/spark-training-v2.0/blob/master/day%201/part%201/shell/exercices/02-pyspark-shell.md)

