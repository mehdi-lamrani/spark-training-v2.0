{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### READING SPARK DATAFRAMES FROM HDFS AND PERFORMING BASIC OPERATIONS\n",
    "\n",
    "- The goal is simply to get acquainted with pyspark on jupyter and warmu a bit\n",
    "- In this workshop we will focus on inferring schemas for a CSV file to have some structure on our dataset  \n",
    "\n",
    "\n",
    "- **Prereqs**  \n",
    "Make sure you restart your context manager before you start a Jupyter Pyspark Session   \n",
    "(just in case there are some previous spark sessions that are still running.)  \n",
    "\n",
    "\n",
    "- CAUTION : careful with that command : It will also kick you pair colleague from any of his active sessions\n",
    "  please synchronize together to avoid that.  \n",
    "  \n",
    "  \n",
    "  On a terminal, type :  \n",
    "    `sudo su`  \n",
    "    `cd`  \n",
    "    `systemctl restart hadoop-yarn-resourcemanager`  \n",
    "\n",
    "\n",
    "- **Explore the DataSets :**  \n",
    "\n",
    "\n",
    "- Reference of the Data : \n",
    "  [Bureau of Transportation Statistics - AIrline Traffic](https://www.transtats.bts.gov/Tables.asp?DB_ID=120&DB_Name=Airline%20On-Time%20Performance%20Data&DB_Short_Name=On-Time#)  \n",
    "  \n",
    "- Take a couple of minutes to acknowledge the dataset data  \n",
    "\n",
    "- The dataset files have been preloaded in your Cluster for your convenience  \n",
    "\n",
    "- Check the data files on HDFS :  \n",
    "    `hdfs dfs -ls /user/root/data/BOTS/CSV`  \n",
    "    `hdfs dfs -ls /user/root/data/BOTS_REF/CSV`  \n",
    "    \n",
    "    \n",
    "<br>\n",
    "\n",
    "\n",
    "- **Start Spark Application**   \n",
    "\n",
    "This is just to turn the spark application on and retrieve an Application ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Define the HDFS Data Directory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('hello pyspark kernel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOTS_DIR_HDFS = \"hdfs:///user/root/data/BOTS/CSV\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Load Dataframe from HDFS**<br>\n",
    "- Load the whole directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\").load(BOTS_DIR_HDFS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Count DataFrame Records**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Print Default DataFrame Schema**\n",
    "- Explore the Schema \n",
    "- What can you say about it ? What are your thoughts ?\n",
    "- Is this Schema ok ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Load Dataframe : Add Header Option**<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(BOTS_DIR_HDFS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Did you Notice any change in the execution time compared to the execution without the header option ?  \n",
    "\n",
    "\n",
    "\n",
    "- **Print DataFrame Schema**\n",
    "- Explore the Schema. What changes do you notice ?\n",
    "- How does this Schema compare to the previous one ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Load Dataframe : add InferSchema Option**<br>\n",
    "- What changes do you notice regarding execution time ?\n",
    "- What are your guesses about that ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(BOTS_DIR_HDFS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Print DataFrame Schema**<br>\n",
    "- What can you say about this Schema ?\n",
    "- Is it better than the previous one ?\n",
    "- Is it perfect ?\n",
    "- Why in your opinion does it take so much time ?\n",
    "- What is actually happening ? How is spark reading data to infer the Schema ?\n",
    "\n",
    "- **Let's try some simple transformations / actions **<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_state = df.select(\"OriginStateName\").distinct()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How much time did that take ?\n",
    "- What did Spark do at this point ?\n",
    "- What is the nature of the distinct operation ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_state.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Why does the show operation take more time than the distinct ?\n",
    "- What is the nature of the show operation ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_state.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monitoring Jobs, Stages, & Tasks\n",
    "\n",
    "In the training markdown guide there is a monitoring guide.   \n",
    "Follow the instructions there to track the jobs, stages, tasks, and the dags generated by your operations, \n",
    "on the Spark Web UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conlusion : \n",
    "\n",
    "As you probably noticed,   \n",
    "We made some progress by infering the Schema, but we are still lacking some sound Schema information.   \n",
    "What are the benefits of having a more robust Schema ?  \n",
    "Can you list some ?  Business wise ? Technically ?   \n",
    "\n",
    "\n",
    "Later we will explore a few options to make for a better structure of our data.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bonus Exercise : \n",
    "*(if time permits)*\n",
    "\n",
    "Use some reference data provided in the BOTS_REF data folder in HDFS  \n",
    "You can try to perform a few sensible joins & filters, play around and display the resulting data sets.   \n",
    "This will be a convenient warmup for tomorrow's Spark SQL analytics workshops  \n",
    "It will also be a good occasion to build transformations and fire some actions, to watch DAG creation and execution.   \n",
    "\n",
    "Useful examples for data exploration & filtering & joining   \n",
    "https://sparkbyexamples.com/pyspark/select-columns-from-pyspark-dataframe/  \n",
    "https://sparkbyexamples.com/pyspark/pyspark-where-filter/  \n",
    "https://sparkbyexamples.com/pyspark/pyspark-groupby-explained-with-example/  \n",
    "https://sparkbyexamples.com/pyspark/pyspark-join-explained-with-examples/  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
