{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leveraging Parquet File Format efficiency & Enforcing better Schemas in our data\n",
    "\n",
    "- **Start Spark Application** <br>\n",
    "This is just to turn the spark application on and retrieve an Application ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Define the HDFS Data Directories that will host parquet files**\n",
    "- **NOTE : As you are working in pairs on the same cluster, if you want both to work simultaneously, you should create each create your parquet folders with specific names and modify the botebook accordingly**\n",
    "  \n",
    "\n",
    "\n",
    "`hdfs dfs -mkdir /user/root/data/BOTS/PARQUET`<br>\n",
    "`hdfs dfs -mkdir /user/root/data/BOTS_REF/PARQUET`<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOTS_DIR_HDFS_CSV = \"hdfs:///user/root/data/BOTS/CSV\"\n",
    "BOTS_DIR_HDFS_PQT = \"hdfs:///user/root/data/BOTS/PARQUET\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Timing Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from datetime import timedelta\n",
    "\n",
    "class T():\n",
    "    def __enter__(self):\n",
    "        self.start = time()\n",
    "    def __exit__(self, type, value, traceback):\n",
    "        self.end = time()\n",
    "        elapsed = self.end - self.start\n",
    "        print(str(timedelta(seconds=elapsed)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Usage : \n",
    "\n",
    "```\n",
    "with T():\n",
    "    //instructions you want to time\n",
    "```\n",
    "Note : %%time magic command does not work with this version of pyspark kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid blue\"></hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attesting Parquet Performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Load a Dataframe :**<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with T():\n",
    "    df_csv = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(BOTS_DIR_HDFS_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with T():\n",
    "    print(df_csv.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Print CSV DataFrame Schema**<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_csv.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Write the Dataframe to Parquet Format**\n",
    "- Use the right dedicated subdirectory and add a sensible filename with a .parquet extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOTS_FILE_HDFS_PQT = BOTS_DIR_HDFS_PQT +\"/\"+\"BOTS_2019.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv.write.parquet(BOTS_FILE_HDFS_PQT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now Explore Parquet Files on HDFS using your SSH Termina\n",
    "\n",
    "`hdfs dfs -ls /user/root/data/BOTS/PARQUET`  \n",
    "`hdfs dfs -ls /user/root/data/BOTS/PARQUET/BOTS_2019.parquet`\n",
    "\n",
    "- What do you observe in the folder ?\n",
    "- How is a parquet File structured (link that to the course) ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Read the Dataframe back from the Parquet File**\n",
    "- Compare reading time with reading from CSV Format\n",
    "- What do you observe ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with T():\n",
    "    df_parquet = spark.read.format(\"parquet\").load(BOTS_FILE_HDFS_PQT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with T():\n",
    "    print(df_parquet.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Print Parquet DataFrame Schema**<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parquet.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What do you notice ?\n",
    "- Where in your opinion were schema information stored ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing performance : \n",
    "Try operations form the previous notebook (distinct(), or try any other transformation like filter, groupBy, or else)  \n",
    "on both `df_csv` and `df_parquet`, compare timings, and share your thoughts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO : FILL IN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid blue\"></hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining & applying better schemas to your dataframe :\n",
    "\n",
    "- Explore your dataframe and the inferrred schema and try to improve the schema  \n",
    "\n",
    "    * A strong schema makes for better performance and better reliability\n",
    "    * Below is an example with a movie dataset that we will be using later\n",
    "    * Try to apply that to your BOTS dataframe\n",
    "    * save back to parquet\n",
    "    * load again from parquet without any inference\n",
    "    * print your schema \n",
    "    * check that your schema was save correctly to parquet\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example with a Movies Dataset \n",
    "\n",
    "````\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
    "````\n",
    "You should add every type you need to this import\n",
    "https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.types\n",
    "\n",
    "\n",
    "````\n",
    "moviesStruct = \n",
    "   [StructField(\"movieId\", IntegerType(), True),\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"genres\", StringType(), True)]\n",
    "\n",
    "moviesSchema = StructType(moviesStruct)\n",
    "````\n",
    "\n",
    "- You will face a small practical problem though : You have a very large number of columns\n",
    "\n",
    "- A Handy alternative is to use datatype strings : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The following CELLS are EXAMPLE Cells  \n",
    "- You need to apply them to you specific dataset (it time permits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
    "\n",
    "data2 = [(\"James\",\"Smith\",\"36636\",\"M\",3000),\n",
    "    (\"Michael\",\"Rose\",\"40288\",\"M\",4000),\n",
    "    (\"Robert\",\"Williams\",\"42114\",\"M\",4000),\n",
    "    (\"Maria\",\"Jones\",\"39192\",\"F\",4000),\n",
    "    (\"Jen\",\"Brown\",\"\",\"F\",2000)\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff = spark.createDataFrame(data=data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# we can use a datatype string - This is not much documented actually, \n",
    "# but way easier, less verbose, and more straightforward than using structypes\n",
    "\n",
    "ddlSchema = (\"FIRSTNAME STRING, NAME STRING, ZIPCODE STRING, GENDER STRING, SALARY INT\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff = spark.createDataFrame(data=data2,schema=ddlSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : FILL IN\n",
    "# APPLY TO YOUR PARQUET DATASET\n",
    "# IF TIME PERMITS"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
