{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BUILDING A MOVIE RECOMMENDATION ENGINE WITH SPARK MLlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Launch Your Spark Application & Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid blue\"></hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MEET MOVIE LENS DATASET \n",
    "\n",
    "- Please take time to get acquainted with the dataset :   \n",
    "  https://grouplens.org/datasets/movielens/  \n",
    "  http://files.grouplens.org/datasets/movielens/ml-latest-README.html  \n",
    "    \n",
    "    \n",
    "- Before writing dataframe queries, you need to understand your dataset  \n",
    "  Some valuable information in the description might save you some time trying to figure things out for yourself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOADING DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download Dataset & Load into HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "sudo su \n",
    "cd \n",
    "mkdir mov\n",
    "cd mov\n",
    "\n",
    "wget http://files.grouplens.org/datasets/movielens/ml-latest.zip\n",
    "unzip ml-latest.zip\n",
    "rm -f ml-latest.zip\n",
    "\n",
    "usermod -a -G hadoop root\n",
    "hdfs dfs -mkdir /user/root/data/\n",
    "hdfs dfs -mkdir /user/root/data/MOV/\n",
    "hdfs dfs -mkdir /user/root/data/MOV/CSV\n",
    "\n",
    "hdfs dfs -put ml-latest/* /user/root/data/MOV/CSV\n",
    "```\n",
    "\n",
    "- Check you files are loaded correctly in HDFS \n",
    "```\n",
    "hdfs dfs -ls /user/root/data/MOV/CSV\n",
    "```\n",
    "\n",
    "- You should have the following output :\n",
    "\n",
    "```\n",
    "[root@ip-172-31-17-80 mov]# hdfs dfs -ls /user/root/data/MOV/CSV\n",
    "Found 7 items\n",
    "-rw-r--r--   1 root hadoop       9784 2020-11-26 23:57 /user/root/data/MOV/CSV/README.txt\n",
    "-rw-r--r--   1 root hadoop  414851573 2020-11-26 23:57 /user/root/data/MOV/CSV/genome-scores.csv\n",
    "-rw-r--r--   1 root hadoop      18103 2020-11-26 23:57 /user/root/data/MOV/CSV/genome-tags.csv\n",
    "-rw-r--r--   1 root hadoop    1267039 2020-11-26 23:57 /user/root/data/MOV/CSV/links.csv\n",
    "-rw-r--r--   1 root hadoop    2858223 2020-11-26 23:57 /user/root/data/MOV/CSV/movies.csv\n",
    "-rw-r--r--   1 root hadoop  759200511 2020-11-26 23:57 /user/root/data/MOV/CSV/ratings.csv\n",
    "-rw-r--r--   1 root hadoop   39744990 2020-11-26 23:57 /user/root/data/MOV/CSV/tags.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INSTALL PANDAS\n",
    "\n",
    "- From the SSH Terminal \n",
    "\n",
    "````\n",
    "pip install pandas\n",
    "````\n",
    "\n",
    "Repeat this step if you need any extra/missing other python library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform a few useful imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "import pandas as pd\n",
    "pd.options.display.max_columns = None\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define schema for Movies Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moviesStruct = [StructField(\"movieId\", IntegerType(), True),\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"genres\", StringType(), True)]\n",
    "\n",
    "moviesSchema = StructType(moviesStruct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read Movies dataframe from HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read movies from HDFS as CSV (FIRST TIME ONLY)\n",
    "\n",
    "moviesDF = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"delimiter\", \",\") \\\n",
    "    .schema(moviesSchema) \\\n",
    "    .load(\"hdfs:///user/root/data/MOV/CSV/movies.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write Movies Dataframe to Parquet File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moviesDF.write.parquet(\"hdfs:///user/root/data/MOV/PARQUET/movies.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reload Movies Dataframe From Parquet File "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (everytime after the first export to PARQUET (in case you restart your kernel/notebook/session))\n",
    "\n",
    "moviesDF = spark.read.parquet(\"hdfs:///user/root/data/MOV/PARQUET/movies.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make Use of Caching Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# caching might be of great help for crossvalidation -among others\n",
    "# read caching comment for ratingsDF below\n",
    "\n",
    "moviesDF.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define schema for Ratings Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define schema for ratings dataset\n",
    "ratingsStruct = [StructField(\"userId\", IntegerType(), True),\n",
    "    StructField(\"movieId\", IntegerType(), True),\n",
    "    StructField(\"rating\", DoubleType(), True),\n",
    "    StructField(\"timestamp\", IntegerType(), True)]\n",
    "\n",
    "ratingsSchema = StructType(ratingsStruct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read Ratings Dataframe from HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read ratings from HDFS (FIRST TIME ONLY)\n",
    "ratingsDF = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"delimiter\", \",\") \\\n",
    "    .schema(ratingsSchema) \\\n",
    "    .load(\"hdfs:///user/root/data/MOV/CSV/ratings.csv\")\n",
    "\n",
    "ratingsDF.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write Ratings Dataframe to Parquet File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratingsDF.write.parquet(\"hdfs:///user/root/data/MOV/PARQUET/ratings.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reload Ratings Dataframe From Parquet File "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# LOAD RATINGS From Parquet File (everytime after the first export to PARQUEY)\n",
    "\n",
    "ratingsDF = spark.read.parquet(\"hdfs:///user/root/data/MOV/PARQUET/ratings.parquet\").drop(\"timestamp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make Use of Caching Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Caching might be of great help - especially for crossvalidation -among others\n",
    "# it is recommended for RDD re-use in iterative machine learning applications\n",
    "# Check the size of your data on disk, and the total memory available to spark\n",
    "# to see how much of your data fits into memory\n",
    "# If the RDD does not fit in memory, some partitions will not be cached and will be recomputed on the fly each time they're needed. \n",
    "\n",
    "ratingsDF.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid blue\"></hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration & Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select Users Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique Users Id :\n",
    "usersDF = ratingsDF.select(\"userId\").distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Total User Count\n",
    "\n",
    "usersDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "usersDF.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group Rating Count by Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ratingsDF.groupBy(\"userId\").count().show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary \n",
    "# Got 1000209 ratings from 6040 users on 3883 movies.\n",
    "print(\"Got {} ratings from {} users on {} movies.\".format(ratingsDF.count(), usersDF.count(), moviesDF.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ratingsDF.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select Max User ID \n",
    "This will be useful for user incrementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ratingsDF.agg({\"userId\": \"max\"}).collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ratingsDF.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Ratings View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratingsDF.createTempView(\"RATING\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use SQL to select Ratings for a specific User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "select * from RATING where userId = 283228"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "select count(*) from RATING where userId = 283228"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a Sample Dataframe \n",
    "(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sampleDF = moviesDF.sample(fraction=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid blue\"></hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Movies Dataset \n",
    "\n",
    "Genres are a pipe-separated list,   \n",
    "and are selected from the following:\n",
    "\n",
    "Action  \n",
    "Adventure  \n",
    "Animation  \n",
    "Children's  \n",
    "Comedy  \n",
    "Crime  \n",
    "Documentary  \n",
    "Drama  \n",
    "Fantasy  \n",
    "Film-Noir  \n",
    "Horror  \n",
    "Musical  \n",
    "Mystery  \n",
    "Romance  \n",
    "Sci-Fi  \n",
    "Thriller  \n",
    "War  \n",
    "Western  \n",
    "(no genres listed)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select Comedy Movies Only\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "moviesDF.filter(moviesDF.genres.contains('Comedy')).limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the TOP rated Comedy Movies (not aggregated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "moviesDF.filter(moviesDF.genres.contains('Comedy')) \\\n",
    "    .join(ratingsDF, \"movieId\") \\\n",
    "    .sort(col(\"rating\").desc()) \\\n",
    "    .show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count of Comedy Movies Grouped By Rating "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "moviesDF.filter(moviesDF.genres.contains('Comedy')) \\\n",
    "    .join(ratingsDF, \"movieId\") \\\n",
    "    .groupBy(col(\"rating\")).count().orderBy(\"rating\") \\\n",
    "    .show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MOST Rated Comedy Movies - No matter the rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "moviesDF.filter(moviesDf.genres.contains('Comedy')) \\\n",
    "    .join(ratingsDF, \"movieId\") \\\n",
    "    .groupBy(col(\"movieId\")).count().orderBy(\"count\", ascending=False) \\\n",
    "    .show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Most Rated Comedy Movies, grouped by Movie & Rating\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "moviesDF.filter(moviesDf.genres.contains('Comedy')) \\\n",
    "    .join(ratingsDF, \"movieId\") \\\n",
    "    .groupBy(col(\"movieId\"),col(\"title\"), col(\"rating\")).count().orderBy(\"count\", ascending=False) \\\n",
    "    .show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top Rated Comedy Movies with most ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "genreMovieDF = moviesDF.filter(moviesDF.genres.contains(\"insert_your_preferred_genre_here\"))\n",
    "    \n",
    "moviesByRating_counts = genreMovieDF \\\n",
    "        .join(ratingsDF, \"movieId\") \\\n",
    "        .groupBy(\"movieId\").count().alias(\"count\").orderBy(desc(\"count\"))\n",
    "        #.groupBy(\"movieId\").count().alias(\"ratings count\").orderBy(desc(\"count\"))\n",
    "\n",
    "moviesByRating_Full = genreMovieDF.join(moviesByRating_counts, \"movieId\") \\\n",
    "                                  .dropDuplicates().orderBy(desc(\"count\"))\n",
    "    \n",
    "movieByRating_full.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "movieByRating_Full.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movieByRating_Full.coalesce(1) \\\n",
    "      .write \\\n",
    "      .option(\"header\",\"true\") \\\n",
    "      .option(\"sep\",\",\") \\\n",
    "      .mode(\"overwrite\") \\\n",
    "      .csv(\"file:///path/output/file\") \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get User Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Cell does not work on Jupyter unfortunately \n",
    "# due to Pyspark limited Python libraries integration\n",
    "# This need py4j integration and a call to scala/java\n",
    "# code for that is provided as a standalone python program in demo\n",
    "\n",
    "\n",
    "for row in sampleDF.rdd.collect():\n",
    "    print(\"Please rate the following movie (1-5 (best), or 0 if not seen):\\n\" + row.title + \":\")\n",
    "    rate = int(input())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other Request Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "joinDF = moviesDF.join(ratingsDF, \"movieId\")\n",
    "joinDF.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ratingsDF.distinct().groupBy(\"rating\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid blue\"></hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODELING PART\n",
    "\n",
    "### Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting data into training & test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainingDF,testDF = ratingsDF.randomSplit([0.8, 0.2], seed=12345)\n",
    "\n",
    "# Got 1000209 ratings from 6040 users on 3883 movies.\n",
    "#print(\"Training {}, test {}.\".format(trainingDF.count(), testDF.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timing Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from datetime import timedelta\n",
    "\n",
    "class T():\n",
    "    def __enter__(self):\n",
    "        self.start = time()\n",
    "    def __exit__(self, type, value, traceback):\n",
    "        self.end = time()\n",
    "        elapsed = self.end - self.start\n",
    "        print(str(timedelta(seconds=elapsed)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainingDF.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "testDF.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingDF.select(\"userId\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDF.select(\"userId\").distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point it is interesting to see that there might be users on the test dataset  \n",
    "that have no occurence in the training dataset, as the figures below can possibly suggest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training ALS model on the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.ml.recommendation import ALS\n",
    "\n",
    "als = ALS(maxIter=5,\n",
    "          regParam=0.01, \n",
    "          implicitPrefs=False, \n",
    "          userCol=\"userId\", \n",
    "          itemCol=\"movieId\", \n",
    "          ratingCol=\"rating\", \n",
    "          coldStartStrategy=\"drop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fit Model (just a test - DO NOT RUN if you still need to cross-validate)\n",
    "\n",
    "with T():\n",
    "    model = als.fit(trainingDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid blue\"></hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation & Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "# WARNING : THIS CAN ESCALATE VERY QUICKLY, EVEN WITH ONLY TWO MODELS TO BE TESTED\n",
    "# UNLESS YOU ARE ABLE TO NARROW DOWN AND LOWER ITERATIONS CONSIDERABLY, \n",
    "# YOU WOULD BETTER AVOID THIS STEP ALL TOGETHER\n",
    "\n",
    "# NOTE THAT USING AN RMSE MINIMIZING LOOP CAN ALSO DO THE JOB\n",
    "\n",
    "#param_grid = ParamGridBuilder() \\\n",
    "#            .addGrid(als.rank, [10, 50, 100, 150]) \\\n",
    "#            .addGrid(als.regParam, [.01, .05, .1, .15]) \\\n",
    "#            .build()\n",
    "\n",
    "#param_grid = ParamGridBuilder() \\\n",
    "#            .addGrid(als.rank, [50, 100]) \\\n",
    "#            .addGrid(als.regParam, [.05, .1]) \\\n",
    "#            .build()\n",
    "\n",
    "param_grid = ParamGridBuilder() \\\n",
    "            .addGrid(als.rank, [50]) \\\n",
    "            .addGrid(als.regParam, [.05]) \\\n",
    "            .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Define evaluator as RMSE and print length of evaluator\n",
    "evaluator = RegressionEvaluator(\n",
    "           metricName=\"rmse\", \n",
    "           labelCol=\"rating\", \n",
    "           predictionCol=\"prediction\") \n",
    "\n",
    "print (\"Num models to be tested: \", len(param_grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CrossValidator(estimator=als, \\\n",
    "                    estimatorParamMaps=param_grid, \\\n",
    "                    evaluator=evaluator, \\\n",
    "                    numFolds=5\n",
    "                    parallelism=4) # this last parallelism param is crucial for increasing performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit The Cross Validator \n",
    "#### *(Optional - VERY Time Consuming)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# WARNING : Running this from a notebook hooked to Spark through Livy \n",
    "# will end up in a timeout after waiting for a long time\n",
    "# the job will still be running but it will be abruptly be killed due to a timeout setting\n",
    "# you need to setup livy.server.session.timeout and extend it to 1h  in livy.conf on the server, and restart livy\n",
    "# livy.server.session.state-retain.sec = 600s => move to 3600s \n",
    "# location : /etc/livy/conf/livy.conf\n",
    "\n",
    "# check spark web UI\n",
    "# check generated DAGs\n",
    "# check memory usage\n",
    "\n",
    "cvModel = cv.fit(trainingDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid blue\"></hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model by computing the RMSE on the Rating Predictions established for test data\n",
    "\n",
    "predictions = model.transform(testDF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainingDF.filter(testDF.userId == \"12\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDF.filter(testDF.userId == \"12\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "testDF.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "testDF.groupBy(\"userId\").count().show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with T():\n",
    "    predictions.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions.groupBy(\"userId\").count().show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate using Regression Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\",\n",
    "                                predictionCol=\"prediction\")\n",
    "\n",
    "# Lower values of RMSE indicate better fit\n",
    "\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root-mean-square error = \" + str(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid blue\"></hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Utilization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate recommendations for each user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAUTION : This takes too much time to compute\n",
    "# Use subsets instead (cell below)\n",
    "\n",
    "# Generate top 10 movie recommendations for each user\n",
    "userRecs = model.recommendForAllUsers(10)\n",
    "\n",
    "# Generate top 10 user recommendations for each movie\n",
    "movieRecs = model.recommendForAllItems(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate recommendations for a subset of user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate top 10 movie recommendations for a specified set of users\n",
    "users = ratingsDF.select(als.getUserCol()).distinct().limit(3)\n",
    "userSubsetRecs = model.recommendForUserSubset(users, 10)\n",
    "\n",
    "# Generate top 10 user recommendations for a specified set of movies\n",
    "movies = ratingsDF.select(als.getItemCol()).distinct().limit(3)\n",
    "movieSubSetRecs = model.recommendForItemSubset(movies, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "userSubsetRecs.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "userSubsetRecs.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid blue\"></hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model recommendation for users based on their rating \n",
    "# You will notice that the Returned Dataframe has nested values \n",
    "# DataFrame[userId: int, recommendations: array<struct<movieId:int,rating:float>>]\n",
    "# You will also notice that predicted ratings are all over the place\n",
    "# This is not strictly an issue as it is not used \"directly\" and it can be ignored. \n",
    "# For more info see: https://stackoverflow.com/q/29051520/426332\n",
    "\n",
    "recoDF = userSubsetRecs.limit(10)\n",
    "#.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Formatting & Join for staging & presentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Denormalize Recommendation Dataframe & explode the inner list so it can be selectable as a column   \n",
    "# and filter out the rating colums as we do not need it\n",
    "\n",
    "recoDF.select(col(\"userId\"),explode(col(\"recommendations\"))) \\\n",
    "      .select(col(\"userId\"),col(\"col.movieId\")) \\\n",
    "      .join(moviesDF,\"movieId\") \\\n",
    "      .orderBy(\"userId\") \\\n",
    "      .select(\"userId\", \"movieId\", \"title\") \\\n",
    "      .toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "movieSubSetRecs.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Recommendations for a specific user\n",
    "aUserId = 12\n",
    "recommandations = userRecs.filter(col(\"userId\") == aUserId)\n",
    "recommandations.show(1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "\n",
    "# Let's flatten the movie recommandations and look in detail\n",
    "userRecommandations = recommandations.select(\n",
    "  explode(col(\"recommendations\").movieId).alias(\"movieId\")\n",
    ")\n",
    "\n",
    "print(\"Recommandations for user {} :\".format(aUserId))\n",
    "\n",
    "moviesDF.join(userRecommandations, \"movieId\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Ratings from the user\n",
    "\n",
    "ratingsDF.filter(col(\"userId\") == aUserId) \\\n",
    "    .join(moviesDF, \"movieId\") \\\n",
    "    .sort(col(\"rating\").desc()) \\\n",
    "    .show(100, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid blue\"></hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Model Experiments with MLFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **Before You do Anything :**\n",
    "#### Make SURE to read CAREFULLY the MLFlow README File first\n",
    "#### Make SURE you have successfully installed MLFlow by following the instructions  \n",
    "<br>\n",
    "\n",
    "- **WARNING :** Missing Any Small Step might result in corrupting your environment.   \n",
    "\n",
    "- **IN CASE OF DOUBT :** Call the Trainer in. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(\"file:///mlruns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mlflow.start_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mlflow.spark.log_model(model, \"ALSmodel_Lite8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model saved in run %s\" % mlflow.active_run().info.run_uuid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run MLFlow UI to check your model**\n",
    "```\n",
    "mlflow ui --backend-store-uri /mlrun --host 0.0.0.0&\n",
    "```\n",
    "\n",
    "go to http://YOUR.IP.ADD.RESS:5000 :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serve Model with MLFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# Console commands\n",
    "# Serve & Curl Request EXAMPLES  \n",
    "(Make SURE to read carefully the MLFLOW README FILE first)\n",
    "\n",
    "# Please understand the syntax and adapt it accordingly to your case\n",
    "# Mind you copy pasting !\n",
    "\n",
    "mlflow models serve -m /mlruns/0/6c8050941d0744b8ac3652ff22d40983/artifacts/ALSmodel_Lite2 -h 0.0.0.0 --port 9999 --no-conda\n",
    "\n",
    "\n",
    "curl -X POST localhost:9999/invocations -H 'Content-Type: application/json; format=pandas-split' -d '{\"columns\":[\"userId\",\"movieId\",\"rating\"], \"data\":[[1311,144210,5,1604450652]]}'  \n",
    "\n",
    "curl --request POST http://localhost:9999/invocations --header 'Content-Type: application/json; format=pandas-split' --data @mlf_data.json\n",
    "\n",
    "curl -X POST http://localhost:9999/invocations -H 'Content-Type: application/json; format=pandas-split' -d @mlf_data.json\n",
    "```\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
